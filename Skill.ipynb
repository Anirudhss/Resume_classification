{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Skill.ipynb","provenance":[],"mount_file_id":"1d_JTTD5ATj2zcdtjh5VcaC4F2AdiVX0B","authorship_tag":"ABX9TyOl9lqMbIjkBQ8H1jtlfF32"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d5c1ce52405149bfb60f9779bf69a4a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2205f9ff77b2438cbc59602f3f7720d3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9fd7c516041b46639ce6870860b01a2e","IPY_MODEL_30369c9eaaef4dce96b693d8707ea587","IPY_MODEL_0334e6f4009f4b728f43b684a6680e7e"]}},"2205f9ff77b2438cbc59602f3f7720d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9fd7c516041b46639ce6870860b01a2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77d49f72036a4c84b231828cf2f23dee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_944d58ecb8244f9999c8a2ad5df2c9c2"}},"30369c9eaaef4dce96b693d8707ea587":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6795f721fe6841d08e2e2e45a3852773","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8a0d281054f4335bc461a68789a6804"}},"0334e6f4009f4b728f43b684a6680e7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1b6087dbe2eb4c46b46353368db42ca4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208k/208k [00:00&lt;00:00, 999kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_746d82f31a0c4d588d17afd27006dc4a"}},"77d49f72036a4c84b231828cf2f23dee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"944d58ecb8244f9999c8a2ad5df2c9c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6795f721fe6841d08e2e2e45a3852773":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c8a0d281054f4335bc461a68789a6804":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b6087dbe2eb4c46b46353368db42ca4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"746d82f31a0c4d588d17afd27006dc4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c5223a8e9064fc491d1292e504d771d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0f84c3fa99004aa7a0c756037aec0880","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bc8840ac7f314d479b29573a4d597f82","IPY_MODEL_6ddda713b4724ea78feff565e825fab3","IPY_MODEL_b718157d13cb4c3e9b1aba04e3b843f3"]}},"0f84c3fa99004aa7a0c756037aec0880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc8840ac7f314d479b29573a4d597f82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dca5f5a6e1c54c29a442e41543fca704","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_653ad8c4c36546d48145dfff6b1a086e"}},"6ddda713b4724ea78feff565e825fab3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e8ca61f3f7e94444be310f5e748a3f3f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3b8460304d62416eab862307f93a6139"}},"b718157d13cb4c3e9b1aba04e3b843f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1155aab77c9a4f8399aefc1f995fe3cd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 673B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c577c8d8b1f04e6fb32c0f5b0bfc23cd"}},"dca5f5a6e1c54c29a442e41543fca704":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"653ad8c4c36546d48145dfff6b1a086e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8ca61f3f7e94444be310f5e748a3f3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3b8460304d62416eab862307f93a6139":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1155aab77c9a4f8399aefc1f995fe3cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c577c8d8b1f04e6fb32c0f5b0bfc23cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a8dde26c7fe5474db451f5aa6289d779":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_21252986ce6c4560bb84a8def01b7b85","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0121f270b7b42ce829754cd24e056d0","IPY_MODEL_d7f6e3dc6e1c4006ba1b1cb1a6c7ebe5","IPY_MODEL_ed5595ab703c45a6abbfc4851119c5d2"]}},"21252986ce6c4560bb84a8def01b7b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0121f270b7b42ce829754cd24e056d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4c2ce35fefd6463981b4f6fae1e049df","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c976c4b2f224410d8e3a5419992f5b6d"}},"d7f6e3dc6e1c4006ba1b1cb1a6c7ebe5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a1253f4233764e3b890c20f2f6ddd086","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_118dc83240a6453e8ca9a984e4d4d613"}},"ed5595ab703c45a6abbfc4851119c5d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_10574a7f8a3b4f92a93130e4d83799ba","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426k/426k [00:00&lt;00:00, 1.38MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a5ba52fd82f4980b8dac768e1b5f2ad"}},"4c2ce35fefd6463981b4f6fae1e049df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c976c4b2f224410d8e3a5419992f5b6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1253f4233764e3b890c20f2f6ddd086":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"118dc83240a6453e8ca9a984e4d4d613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"10574a7f8a3b4f92a93130e4d83799ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0a5ba52fd82f4980b8dac768e1b5f2ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68f7ad27327b4b5e804611968637d0fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dd205dc8de1e466e853333addceb0191","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dcd318deaebc452085d010acc606c0ce","IPY_MODEL_2e78104df4614e8c9242b09745d9be71","IPY_MODEL_eb19dc6e46f24f6794ffcb0325dd3b09"]}},"dd205dc8de1e466e853333addceb0191":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcd318deaebc452085d010acc606c0ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_105d7c366c6d4d928203598959dd5003","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_08bd9a6bb2e14bc696c5804ed3e8e41c"}},"2e78104df4614e8c9242b09745d9be71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_50f83cb8d9334fcfad8ab1b831473cf9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5d78b10826014a11849aef1473ed3f0a"}},"eb19dc6e46f24f6794ffcb0325dd3b09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_528fba91d61b4760b39b251aa5e744b4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 11.8kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fe67af154d694891974b0e4336865b5d"}},"105d7c366c6d4d928203598959dd5003":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"08bd9a6bb2e14bc696c5804ed3e8e41c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50f83cb8d9334fcfad8ab1b831473cf9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5d78b10826014a11849aef1473ed3f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"528fba91d61b4760b39b251aa5e744b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fe67af154d694891974b0e4336865b5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"edf96caaf69c4c0ab1cc1ca15314cde7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_33cf6f040d124b7282cd5b0129e18be1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_776952c8fe7e418a9ab23fba4b99cdd0","IPY_MODEL_e4e13553a15c472abdc0874a357b3f0e","IPY_MODEL_14e17ed8b0294e1ea5775e0d8609325d"]}},"33cf6f040d124b7282cd5b0129e18be1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"776952c8fe7e418a9ab23fba4b99cdd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f488c3fb04354aa88925d19d672c490e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ef7255a81ce445da96b5458e015b16a"}},"e4e13553a15c472abdc0874a357b3f0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_082e1c848a2c478e8fc98ad31ac391a2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1012326abf0246d082a4772f369bb20a"}},"14e17ed8b0294e1ea5775e0d8609325d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9fa4273698224d1e90026ec2b706bc23","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 416M/416M [00:15&lt;00:00, 27.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c297420fe71d4aef8ec1da02d6e50df0"}},"f488c3fb04354aa88925d19d672c490e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ef7255a81ce445da96b5458e015b16a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"082e1c848a2c478e8fc98ad31ac391a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1012326abf0246d082a4772f369bb20a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9fa4273698224d1e90026ec2b706bc23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c297420fe71d4aef8ec1da02d6e50df0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"mPctKF3NQBox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633457834873,"user_tz":-330,"elapsed":9871,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"1ce7277d-bb38-49f2-cd25-b7ee9c2cec55"},"source":["!pip install -qq transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.9 MB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 636 kB 42.3 MB/s \n","\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 46.3 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 39.8 MB/s \n","\u001b[K     |████████████████████████████████| 109 kB 42.6 MB/s \n","\u001b[K     |████████████████████████████████| 546 kB 41.6 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lKTRHmjAaC9","executionInfo":{"status":"ok","timestamp":1633651986322,"user_tz":-330,"elapsed":369,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"9e505081-c2ad-4002-e647-95a40a0ee90a"},"source":["!nvidia-smi -L"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla K80 (UUID: GPU-e476b6a4-b639-6b86-5491-b6c684cf7c27)\n"]}]},{"cell_type":"code","metadata":{"id":"VZdswQ_XBIEa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633457834876,"user_tz":-330,"elapsed":18,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"d1d26bb3-05ac-4d4b-b243-a7b5efd5ec95"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Oct  5 18:17:13 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   73C    P8    35W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"oXuCi5kKAb0c","executionInfo":{"status":"ok","timestamp":1633652017507,"user_tz":-330,"elapsed":25496,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}}},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBtaB9yHAsLs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633457844415,"user_tz":-330,"elapsed":4802,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"d9dc1130-b951-4d7d-b245-3a98234e7c32"},"source":["!pip install watermark\n","%reload_ext watermark\n","%watermark -v -p numpy,pandas,torch,transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting watermark\n","  Downloading watermark-2.2.0-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from watermark) (5.5.0)\n","Collecting importlib-metadata<3.0\n","  Downloading importlib_metadata-2.1.1-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<3.0->watermark) (3.6.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (2.6.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (5.1.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (1.0.18)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->watermark) (0.7.0)\n","Installing collected packages: importlib-metadata, watermark\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.8.1\n","    Uninstalling importlib-metadata-4.8.1:\n","      Successfully uninstalled importlib-metadata-4.8.1\n","Successfully installed importlib-metadata-2.1.1 watermark-2.2.0\n","Python implementation: CPython\n","Python version       : 3.7.12\n","IPython version      : 5.5.0\n","\n","numpy       : 1.19.5\n","pandas      : 1.1.5\n","torch       : 1.9.0+cu102\n","transformers: 4.11.2\n","\n"]}]},{"cell_type":"code","metadata":{"id":"FmoivtxDB-_V"},"source":["import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","from textwrap import wrap\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import GradScaler, autocast"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PspLh2P4CXer","executionInfo":{"status":"ok","timestamp":1633457848169,"user_tz":-330,"elapsed":1985,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"d64fbf82-617b-4b01-b8b2-4c5fc757cc2f"},"source":["data_table = pd.read_csv(\"/content/drive/MyDrive/jalop/resume/classification/Trainingdata.csv\")\n","data_table.head"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method NDFrame.head of                       Labels                                             Resume\n","0                    Dot_Net  .Net Developer;CTO and Front End Developer;Sof...\n","1                    Dot_Net  .Net Developer;Full Stack .Net Developer;.Net ...\n","2                    Dot_Net  .Net Developer;Software Developer;DIRECTOR OF ...\n","3                    Dot_Net  .NET Developer;.NET Developer;Software Enginee...\n","4                    Dot_Net  .NET Developer;Software Developer;Software Dev...\n","...                      ...                                                ...\n","1135  Database_Administrator  Database Administrator;Management Information ...\n","1136  Database_Administrator  Database Administrator;Graduate Assistant;Soft...\n","1137  Database_Administrator  Database Administrator I;Database Administrato...\n","1138  Database_Administrator  Database Administrator;Oracle/SQL DBA;Oracle D...\n","1139  Database_Administrator  Database Administrator;SQL Server Database Adm...\n","\n","[1140 rows x 2 columns]>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scPgb119CttF","executionInfo":{"status":"ok","timestamp":1633457848171,"user_tz":-330,"elapsed":36,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"3292a56c-7d4c-45ae-809c-cf93a884fd5b"},"source":["data_table['Labels'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Andriod_Developer                53\n","Front_Enddeveloper               49\n","Python_Developer                 49\n","Full_Stackdeveloper              48\n","Oracle_Database_Administrator    48\n","UI_Developer                     48\n","Web_Developer                    47\n","Software_Developer               47\n","Security_Analyst                 46\n","Dot_Net                          46\n","Database_Administrator           46\n","Network_Administrator            46\n","Application_Developer            45\n","Software_Engineer                42\n","Cyber_Security_Analyst           41\n","System_Administrator             39\n","SQL_Database_Administration      38\n","Infromation_Security_Analyst     38\n","Project_Manager                  37\n","IT_Analyst                       35\n","PythonDjango_Developer           35\n","Java_Developer                   34\n","Salesforce_Developer             34\n","Data_System_Administrator        33\n","Linux_SystemAdmin                32\n","Hadoop_Developer                 31\n","IT_Program_Manager               29\n","IOS_developer                    24\n","Name: Labels, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xh8nGgCUyvoH","executionInfo":{"status":"ok","timestamp":1633457848172,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"29d38cfa-b1cb-4ea8-c07a-cbb0d121999b"},"source":["class_names = ['Andriod_Developer', 'Python_Developer', 'Front_Enddeveloper','Oracle_Database_Administrator','UI_Developer','Full_Stackdeveloper','Web_Developer','Software_Developer','Database_Administrator','Dot_Net','Network_Administrator','Security_Analyst','Application_Developer','Software_Engineer','Cyber_Security_Analyst','System_Administrator','SQL_Database_Administration','Infromation_Security_Analyst','Project_Manager','IT_Analyst','PythonDjango_Developer','Salesforce_Developer','Java_Developer','Data_System_Administrator','Linux_SystemAdmin','Hadoop_Developer','IT_Program_Manager','IOS_developer']\n","possible_labels = data_table.Labels.unique()\n","\n","label_dict = {}\n","for index, possible_label in enumerate(class_names):\n","    label_dict[possible_label] = index\n","label_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Andriod_Developer': 0,\n"," 'Application_Developer': 12,\n"," 'Cyber_Security_Analyst': 14,\n"," 'Data_System_Administrator': 23,\n"," 'Database_Administrator': 8,\n"," 'Dot_Net': 9,\n"," 'Front_Enddeveloper': 2,\n"," 'Full_Stackdeveloper': 5,\n"," 'Hadoop_Developer': 25,\n"," 'IOS_developer': 27,\n"," 'IT_Analyst': 19,\n"," 'IT_Program_Manager': 26,\n"," 'Infromation_Security_Analyst': 17,\n"," 'Java_Developer': 22,\n"," 'Linux_SystemAdmin': 24,\n"," 'Network_Administrator': 10,\n"," 'Oracle_Database_Administrator': 3,\n"," 'Project_Manager': 18,\n"," 'PythonDjango_Developer': 20,\n"," 'Python_Developer': 1,\n"," 'SQL_Database_Administration': 16,\n"," 'Salesforce_Developer': 21,\n"," 'Security_Analyst': 11,\n"," 'Software_Developer': 7,\n"," 'Software_Engineer': 13,\n"," 'System_Administrator': 15,\n"," 'UI_Developer': 4,\n"," 'Web_Developer': 6}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"fDcTk_vtzAtT"},"source":["data_table['Labels'] = data_table.Labels.replace(label_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2e1lbQ_yF5Os"},"source":["PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d5c1ce52405149bfb60f9779bf69a4a8","2205f9ff77b2438cbc59602f3f7720d3","9fd7c516041b46639ce6870860b01a2e","30369c9eaaef4dce96b693d8707ea587","0334e6f4009f4b728f43b684a6680e7e","77d49f72036a4c84b231828cf2f23dee","944d58ecb8244f9999c8a2ad5df2c9c2","6795f721fe6841d08e2e2e45a3852773","c8a0d281054f4335bc461a68789a6804","1b6087dbe2eb4c46b46353368db42ca4","746d82f31a0c4d588d17afd27006dc4a","0c5223a8e9064fc491d1292e504d771d","0f84c3fa99004aa7a0c756037aec0880","bc8840ac7f314d479b29573a4d597f82","6ddda713b4724ea78feff565e825fab3","b718157d13cb4c3e9b1aba04e3b843f3","dca5f5a6e1c54c29a442e41543fca704","653ad8c4c36546d48145dfff6b1a086e","e8ca61f3f7e94444be310f5e748a3f3f","3b8460304d62416eab862307f93a6139","1155aab77c9a4f8399aefc1f995fe3cd","c577c8d8b1f04e6fb32c0f5b0bfc23cd","a8dde26c7fe5474db451f5aa6289d779","21252986ce6c4560bb84a8def01b7b85","e0121f270b7b42ce829754cd24e056d0","d7f6e3dc6e1c4006ba1b1cb1a6c7ebe5","ed5595ab703c45a6abbfc4851119c5d2","4c2ce35fefd6463981b4f6fae1e049df","c976c4b2f224410d8e3a5419992f5b6d","a1253f4233764e3b890c20f2f6ddd086","118dc83240a6453e8ca9a984e4d4d613","10574a7f8a3b4f92a93130e4d83799ba","0a5ba52fd82f4980b8dac768e1b5f2ad","68f7ad27327b4b5e804611968637d0fa","dd205dc8de1e466e853333addceb0191","dcd318deaebc452085d010acc606c0ce","2e78104df4614e8c9242b09745d9be71","eb19dc6e46f24f6794ffcb0325dd3b09","105d7c366c6d4d928203598959dd5003","08bd9a6bb2e14bc696c5804ed3e8e41c","50f83cb8d9334fcfad8ab1b831473cf9","5d78b10826014a11849aef1473ed3f0a","528fba91d61b4760b39b251aa5e744b4","fe67af154d694891974b0e4336865b5d"]},"id":"petGMdZaGOrb","executionInfo":{"status":"ok","timestamp":1633457849792,"user_tz":-330,"elapsed":1631,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"a3057151-840e-448f-e0c5-d34668105535"},"source":["tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5c1ce52405149bfb60f9779bf69a4a8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c5223a8e9064fc491d1292e504d771d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8dde26c7fe5474db451f5aa6289d779","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68f7ad27327b4b5e804611968637d0fa","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jon-p1BfGgmO","executionInfo":{"status":"ok","timestamp":1633457849793,"user_tz":-330,"elapsed":29,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"22e92e90-740f-431e-cb46-1f9ce7e834cd"},"source":["MAX_LEN = 512\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f4eaa893e70>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"xB6JUbyQGrGF"},"source":["class SkillsDataset(Dataset):\n","  def __init__(self, reviews, targets, tokenizer, max_len):\n","    self.reviews = reviews\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  def __len__(self):\n","    return len(self.reviews)\n","  def __getitem__(self, item):\n","    review = str(self.reviews[item])\n","    target = self.targets[item]\n","    encoding = self.tokenizer.encode_plus(\n","      review,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","    return {\n","      'review_text': review,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'targets': torch.tensor(target, dtype=torch.long)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7an4hr8uHwFS"},"source":["df_train, df_test = train_test_split(\n","  data_table,\n","  test_size=0.2,\n","  random_state=RANDOM_SEED\n",")\n","df_val, df_test = train_test_split(\n","  df_test,\n","  test_size=0.5,\n","  random_state=RANDOM_SEED\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AqT9MN-IYsz","executionInfo":{"status":"ok","timestamp":1633457849796,"user_tz":-330,"elapsed":23,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"42281fcb-1364-4698-8238-fdd007637fed"},"source":["df_train.shape, df_val.shape, df_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((912, 2), (114, 2), (114, 2))"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"GvNANJu_IjAO"},"source":["def create_data_loader(data_table, tokenizer, max_len, batch_size):\n","  ds = SkillsDataset(\n","    reviews=data_table.Resume.to_numpy(),\n","    targets=data_table.Labels.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=2\n","  )\n","BATCH_SIZE = 8\n","scaler = GradScaler()\n","gradient_accumulations = 16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8olRaU-Joe3"},"source":["train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["edf96caaf69c4c0ab1cc1ca15314cde7","33cf6f040d124b7282cd5b0129e18be1","776952c8fe7e418a9ab23fba4b99cdd0","e4e13553a15c472abdc0874a357b3f0e","14e17ed8b0294e1ea5775e0d8609325d","f488c3fb04354aa88925d19d672c490e","0ef7255a81ce445da96b5458e015b16a","082e1c848a2c478e8fc98ad31ac391a2","1012326abf0246d082a4772f369bb20a","9fa4273698224d1e90026ec2b706bc23","c297420fe71d4aef8ec1da02d6e50df0"]},"id":"pX5sd5fzKnzw","executionInfo":{"status":"ok","timestamp":1633457867124,"user_tz":-330,"elapsed":17344,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"dd49cbf4-bcb8-4705-cee3-72eeda6556f0"},"source":["bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edf96caaf69c4c0ab1cc1ca15314cde7","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"id":"f7UH6HTOLhDJ"},"source":["class MainClassifier(nn.Module):\n","  def __init__(self, n_classes):\n","    super(MainClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","    self.drop = nn.Dropout(p=0.25)\n","    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask,\n","      return_dict=False\n","    )\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXoJyUJjL1k7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633457877526,"user_tz":-330,"elapsed":10426,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"8d704c03-0e40-4010-a67e-e9f0af1ff0aa"},"source":["model = MainClassifier(len(class_names))\n","model = model.to(device)\n","EPOCHS = 9"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"id":"uyXhXxFeMIrb"},"source":["optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_Or3ew2MkWj"},"source":["def train_epoch(\n","  model,\n","  data_loader,\n","  loss_fn,\n","  optimizer,\n","  device,\n","  scaler,\n","  scheduler,\n","  n_examples,\n","):\n","  model = model.train()\n","  losses = []\n","  correct_predictions = 0\n","  for i,d in enumerate(data_loader):\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device)\n","    with autocast():\n","      outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, targets)\n","      correct_predictions += torch.sum(preds == targets)\n","      losses.append(loss.item()) \n","    scaler.scale(loss / gradient_accumulations).backward()\n","    if (i + 1) % gradient_accumulations == 0:\n","       scaler.step(optimizer)\n","       scaler.update()\n","       scheduler.step()\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2KkZA5oxNVcy"},"source":["def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","      with autocast():\n","        outputs = model(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask\n","        )\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, targets)\n","        correct_predictions += torch.sum(preds == targets)\n","        losses.append(loss.item())\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-A5RpgaNdlS","executionInfo":{"status":"ok","timestamp":1633460733005,"user_tz":-330,"elapsed":2855522,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"0d6c3c5c-bf4d-4711-e316-292112069e48"},"source":["%%time\n","history = defaultdict(list)\n","best_accuracy = 0\n","for epoch in range(EPOCHS):\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scaler,\n","    scheduler,\n","    len(df_train)\n","  )\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device,\n","    len(df_val)\n","  )\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","  if val_acc > best_accuracy:\n","    best_accuracy = val_acc\n","    torch.save(model.state_dict(), 'best_model.bin')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 3.391470925849781 accuracy 0.04057017543859649\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 3.3514973958333334 accuracy 0.017543859649122806\n","\n","Epoch 2/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 3.2645445706551537 accuracy 0.0625\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 3.081787109375 accuracy 0.06140350877192982\n","\n","Epoch 3/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 2.955677768640351 accuracy 0.12061403508771928\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 2.7118326822916665 accuracy 0.17543859649122806\n","\n","Epoch 4/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 2.5383600603070176 accuracy 0.23684210526315788\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 2.1896565755208335 accuracy 0.3771929824561403\n","\n","Epoch 5/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 2.046224493729441 accuracy 0.4057017543859649\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.7418497721354167 accuracy 0.43859649122807015\n","\n","Epoch 6/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 1.572130705180921 accuracy 0.6206140350877193\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.246356201171875 accuracy 0.7456140350877193\n","\n","Epoch 7/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 1.1085159569455867 accuracy 0.8322368421052632\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.819024658203125 accuracy 0.894736842105263\n","\n","Epoch 8/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.6751264605605811 accuracy 0.9528508771929824\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.4470270792643229 accuracy 0.9736842105263157\n","\n","Epoch 9/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.38173297413608487 accuracy 0.9824561403508771\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.27136027018229164 accuracy 0.9736842105263157\n","\n","CPU times: user 30min 20s, sys: 16min 40s, total: 47min\n","Wall time: 47min 35s\n"]}]},{"cell_type":"code","metadata":{"id":"VIYVbsdpmaUV"},"source":["%cp /content/best_model.bin /content/drive/MyDrive/jalop/resume/classification/best_model.bin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iv9xSCt23sHU","executionInfo":{"status":"ok","timestamp":1633460807728,"user_tz":-330,"elapsed":2238,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"eed091f6-7e99-469c-dfb4-ca910b7f0544"},"source":["trainedmodel = MainClassifier(len(class_names))\n","trainedmodel.load_state_dict = torch.load(\"best_model.bin\")\n","trainedmodel.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["MainClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (drop): Dropout(p=0.25, inplace=False)\n","  (out): Linear(in_features=768, out_features=28, bias=True)\n",")"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"03CUY3IC6y1U","executionInfo":{"status":"ok","timestamp":1633460737911,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"a21c4619-b695-40e1-ba53-475b9b8656dd"},"source":["plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e9JgUBoobdQpJdQQy8ioIsooCAiqAgi2F27rrpi3Z+rLovdRUVB6SACiqAoLKKCEHqvAQKhhZJAenJ+f9yBDTGBJMxkksz5PI8PM3fufe8JhvfMfe97zyuqijHGGN/l5+0AjDHGeJclAmOM8XGWCIwxxsdZIjDGGB9nicAYY3ycJQJjjPFxlghMkSYi34vIXe7eN5cx9BCRqEt8/rGI/N3d5zUmp8SeIzAFjYiczfC2JJAEpLne36uqU/I/qrwTkR7AV6pa8wrbiQTuUdUl7ojLmPMCvB2AMZmpaqnzry/V+YlIgKqm5mdshZX9XZlLsaEhU2icH2IRkWdE5AjwuYiEiMi3InJcRE65XtfMcMwyEbnH9XqEiKwQkbdd++4TkevzuG9dEVkuInEiskREPhCRry4T/xMickxEokVkZIbtX4jIa67XFV0/w2kROSkiv4iIn4h8CdQCFojIWRF52rV/fxHZ4tp/mYg0ydBupOvvaiNwTkSeEpE5mWJ6V0Teycv/D1N0WCIwhU1VoDxQGxiD8zv8uet9LSABeP8Sx3cAdgAVgTeBz0RE8rDvVOAPoALwEnBnDuIuC9QARgEfiEhIFvs9AUQBlYAqwHOAquqdwAGgn6qWUtU3RaQhMA141LX/QpxEUSxDe0OBG4BywFdAHxEpB85VAnAbMPkysZsizhKBKWzSgbGqmqSqCaoao6pzVDVeVeOA14GrL3H8flX9RFXTgElANZwON8f7ikgtoB3woqomq+oKYP5l4k4BXlHVFFVdCJwFGmWzXzWgtmvfXzT7G3lDgO9U9UdVTQHeBkoAnTPs866qHnT9XUUDy4HBrs/6ACdUNeIysZsizhKBKWyOq2ri+TciUlJE/iMi+0UkFqejKyci/tkcf+T8C1WNd70slct9qwMnM2wDOHiZuGMyjdHHZ3Pet4DdwA8isldEnr1Em9WB/RliTHfFUeMScU0C7nC9vgP48jJxGx9gicAUNpm/HT+B8826g6qWAbq7tmc33OMO0UB5ESmZYVuoOxpW1ThVfUJVrwL6A4+LSK/zH2fa/TDOkBgArmGrUOBQxiYzHfMN0EJEmgM3AoVqBpbxDEsEprArjXNf4LSIlAfGevqEqrofWAO8JCLFRKQT0M8dbYvIjSJS39Wpn8GZNpvu+vgocFWG3WcCN4hILxEJxEmKScBvl4g9EZiN6x6Hqh5wR9ymcLNEYAq78Tjj4ieAlcCifDrv7UAnIAZ4DZiB0wlfqQbAEpx7CL8DH6rqUtdn/we84Joh9KSq7sAZ3nkP5+fvh3MzOfky55gEhGHDQsbFHigzxg1EZAawXVU9fkVypVw3u7cDVVU11tvxGO+zKwJj8kBE2olIPdcc/z7AAJzx9wJNRPyAx4HplgTMeR5LBCIy0fXwzOZsPhfXwyy7RWSjiLTxVCzGeEBVYBnOEM67wP2qus6rEV2GiAQDscC15MO9FFN4eGxoSES64/wjmayqzbP4vC/wMNAX58Gdd1S1g0eCMcYYky2PXRGo6nLg5CV2GYCTJFRVV+LM/a7mqXiMMcZkzZtF52pw8cMuUa5t0Zl3FJExOOUECA4Obtu4ceN8CdAYY4qKiIiIE6paKavPCkX1UVWdAEwACA8P1zVr1ng5ImOMKVxEZH92n3lz1tAhLn4asyYXPxFpjDEmH3jzimA+8JCITMe5WXzGVRTLGGMMEJuYwq6jcew8epYdR+Lo17I6bWtnVbT2yngsEYjINKAHUFGcZfrGAoEAqvoxTsncvjgFtuKBkVm3ZIwxRdu5pFR2HTvLzqNx7DwSx85jZ9l1NI7oMxfqK1Ii0J+m1csUrkSgqkMv87kCD7rjXCkpKURFRZGYmHj5nY1PCAoKombNmgQGBno7FFNYqML+32D9VEiO88gp0tKVs0mpxCWmEpeUytnEVOISU0hIcVZiDQbaitCjeAClggIoFRpA6eKBlA4KoESgP1LuLtxU3/AiheJm8eVERUVRunRp6tSpQ/ZrjBhfoarExMQQFRVF3bp1vR2OKejSUmHbPPjtPTi8DoLKQukrm8merkpKmpKUmkZyajrJqekkpaaTkpZ+YZ8Agcr+ftQM8KdYkB/FA/woFuBHoL/fxaVzk/hfFavE01cUV3aKRCJITEy0JGAuEBEqVKjA8ePHvR2KKciS4mDtl7DyIzhzAMrXgxvGQcuhUKzk5Y8HUtLSiTxxjh2ucfxdR+PYcTSO/THxpKU7D+v6+wl1KwbTsEopGlQuTaOqpWlYpRS1KwQT6F8wqvwUiUQAWBIwF7HfB5Ot2MOw6mNY8wUknYFaneH6N6Dh9eCXdcecmpbO/pPx/7txezSOXUfj2HfiHClpTocvAnUqBNOgcin6Nq9GQ1eHX7diMMUDslsnqWAoMonAGGMu6cgm+O192DwbNB2aDoBOD0PNthftFn0mgc2HYp0bt66Of8/xsySn/m9YJ7R8CRpWLk3PxlVoVNX5pl+/cimCAgt2h58dSwRucPr0aaZOncoDDzyQ62P79u3L1KlTKVeunAciM8bHqcKen5zx/73LIDAY2o2GjvdBSJ2Ldk1KTWPcjzuZsHwv50uwVS8bRIMqpenWoCINKpeiYRWnww8uXrS6zqL103jJ6dOn+fDDD7NMBKmpqQQEZP/XvHDhQk+Glmeqiqril82lsjEFWmoSbJoNv78Px7Y6N397vwRtR0CJP0+/3Ho4lsdnrmf7kTiGtg/llrahNKhSijJBvjHrzP6Vu8Gzzz7Lnj17aNWqFU899RTLli2jW7du9O/fn6ZNmwJw00030bZtW5o1a8aECRMuHFunTh1OnDhBZGQkTZo0YfTo0TRr1ozrrruOhISEP51rwYIFdOjQgdatW9O7d2+OHj0KwNmzZxk5ciRhYWG0aNGCOXPmALBo0SLatGlDy5Yt6dXLWfr2pZde4u23377QZvPmzYmMjCQyMpJGjRoxfPhwmjdvzsGDB7n//vsJDw+nWbNmjB37v8rFq1evpnPnzrRs2ZL27dsTFxdH9+7dWb9+/YV9unbtyoYNG9z4N23MZcSfhF/+BePDYN4DgMBNH8NfN0LXx/6UBNLSlQ+X7WbAByuIOZfM5yPa8X8DW9C2dojPJAEoglcELy/YwtbD7l1vo2n1Mozt1yzbz9944w02b958oRNctmwZa9euZfPmzRemL06cOJHy5cuTkJBAu3btGDRoEBUqVLionV27djFt2jQ++eQTbr31VubMmcMdd9xx0T5du3Zl5cqViAiffvopb775Jv/617949dVXKVu2LJs2bQLg1KlTHD9+nNGjR7N8+XLq1q3LyZOXKgb7vxgmTZpEx44dAXj99dcpX748aWlp9OrVi40bN9K4cWOGDBnCjBkzaNeuHbGxsZQoUYJRo0bxxRdfMH78eHbu3EliYiItW7bM+V+0MXl1cp8z+2fdl5ASD/V6ws0fw1XXOHdxs7A/5hxPzNzAmv2n6BtWldduCqN8cLF8DrxgKHKJoKBo3779RXPY3333XebOnQvAwYMH2bVr158SQd26dWnVqhUAbdu2JTIy8k/tRkVFMWTIEKKjo0lOTr5wjiVLljB9+vQL+4WEhLBgwQK6d+9+YZ/y5ctfNu7atWtfSAIAM2fOZMKECaSmphIdHc3WrVsREapVq0a7du0AKFOmDACDBw/m1Vdf5a233mLixImMGDHisucz5oocXA2/vwfbFoD4Q9hg6PQgVP3TEigXqCrT/jjIa99txd9PGD+kFQNaVffpmWZFLhFc6pt7fgoODr7wetmyZSxZsoTff/+dkiVL0qNHjyyfgi5evPiF1/7+/lkODT388MM8/vjj9O/fn2XLlvHSSy/lOraAgADS0/83AyJjLBnj3rdvH2+//TarV68mJCSEESNGXPLp7ZIlS3Lttdcyb948Zs6cSURERK5jM+ay0tNgx0JnBtDBlc4DYF3+Cu3vhTKXfhDsWGwiz8zZyNIdx+lavyJv3tKC6uVK5FPgBZfdI3CD0qVLExeX/SPpZ86cISQkhJIlS7J9+3ZWrlyZ53OdOXOGGjVqADBp0qQL26+99lo++OCDC+9PnTpFx44dWb58Ofv27QO4MDRUp04d1q5dC8DatWsvfJ5ZbGwswcHBlC1blqNHj/L9998D0KhRI6Kjo1m9ejUAcXFxpKamAnDPPffwyCOP0K5dO0JC3F8Txfiw5HhY/Sm8Hw4z7oC4aOjzT3hsq3Mj+DJJ4LuN0Vw3fjm/743h5f7NmHx3e0sCLkXuisAbKlSoQJcuXWjevDnXX389N9xww0Wf9+nTh48//pgmTZrQqFGji4Zecuull15i8ODBhISE0LNnzwud+AsvvMCDDz5I8+bN8ff3Z+zYsQwcOJAJEyYwcOBA0tPTqVy5Mj/++CODBg1i8uTJNGvWjA4dOtCwYcMsz9WyZUtat25N48aNCQ0NpUuXLgAUK1aMGTNm8PDDD5OQkECJEiVYsmQJpUqVom3btpQpU4aRI62GoHGTs8fgjwmw+jNIOAk12sLgL6BxP/C/fBd2Jj6FsfM38836w7SsWZZxQ1pRr1Ipz8ddiHhszWJPyWphmm3bttGkSRMvRWQyOnz4MD169GD79u1en3pqvxeF3LHtzvTPjTMhLRka3wCdHoJaHbO9AZzZil0neHLWBk6cTeLhng148Jp6BBSQsg75TUQiVDU8q8/sisC4zeTJk3n++ecZN26c15OAKaRUIfIXZ/x/12IICILWdzg3gCvUy3EzCclp/HPRdr74LZJ6lYKZMLwzLWraQ5vZsURg3Gb48OEMHz7c22GYwigtBbZ848wAit4AwZXgmuchfBQEV7j88RmsP3iax2esZ++Jc9zdpS5P92lUaEs/5BdLBMYY70mMhbWTYOXHEBsFFRtCv3ehxRAIDMpVUylp6bz3824+WLqbKqWLM/WeDnSuX9FDgRctlgiMMfkv+RwsewMivoCkWKjTDW4cB/WvzbYC6KXsOhrHYzPXs/lQLAPb1OCl/s186sngK2WJwBiTv2KjYdptzhBQ80HQ+SGo3jpPTaWnK5//Fsk/F22nVPEAPr6jDX2aX9miMr7IEoExJv8c2QRTh0DCaRg6DRpdn+emDp1O4MmZG/h9bwy9m1TmHwPDqFw6d8NJxmFTO7ykVClnHvPhw4e55ZZbstynR48eZJ4qm9n48eOJj4+/8L5v376cPu2Z5eyMuSI7F8PEPs7MoLu/z3MSUFVmR0TR59/L2Rh1mjcHteCT4eGWBK6AJQIvq169OrNnz87z8ZkTwcKFCwvV2gaqelG5C1NErfqPMxxU/ioY/RNUy1sxwpizSdz3VQRPztpAk2plWPRod25tF+rTdYLcwRKBGzz77LMXlXc4X+b57Nmz9OrVizZt2hAWFsa8efP+dGxkZCTNmzsFshISErjtttto0qQJN99880W1hrIqB/3uu+9y+PBhrrnmGq655hrgf2WtAcaNG0fz5s1p3rw548ePv3A+K3dt8k1aKix8Cr5/Ghr2gbsXQZnqeWrqx61H+cv45Szdfpzn+jZm2piOhJbP2drC5tKK3j2C7591xiHdqWqYs6ZpNoYMGcKjjz7Kgw8+CDgVOxcvXkxQUBBz586lTJkynDhxgo4dO9K/f/9sv7189NFHlCxZkm3btrFx40batGlz4bOsykE/8sgjjBs3jqVLl1Kx4sXT5CIiIvj8889ZtWoVqkqHDh24+uqrCQkJsXLXJn8kxsLsu2H3j84Twde+An65n88fl5jCq99uZeaaKJpUK8OUe1rRqGppDwTsu4peIvCC1q1bc+zYMQ4fPszx48cJCQkhNDSUlJQUnnvuOZYvX46fnx+HDh3i6NGjVK1aNct2li9fziOPPAJAixYtaNGixYXPsioHnfHzzFasWMHNN998oZrowIED+eWXX+jfv7+Vuzaed/qgc1P4+Ha48d8Qfneemlm1N4YnZm3g8OkEHuhRj0d7N6RYgA1kuFvRSwSX+ObuSYMHD2b27NkcOXKEIUOGADBlyhSOHz9OREQEgYGB1KlT55JlnLOT23LQl2Plro1HHYqAaUMhJQFunwX1e+W6icSUNP71ww4+XbGPWuVLMuu+TrStffkvGCZvLLW6yZAhQ5g+fTqzZ89m8ODBgFMyunLlygQGBrJ06VL2799/yTa6d+/O1KlTAdi8eTMbN24Esi8HDdmXwO7WrRvffPMN8fHxnDt3jrlz59KtW7cc/zxW7trkydb58PkN4F8cRv2QpySw5fAZBrz/K5/8so9h7Wux8JFulgQ8rOhdEXhJs2bNiIuLo0aNGlSr5jzQcvvtt9OvXz/CwsIIDw+ncePGl2zj/vvvZ+TIkTRp0oQmTZrQtm1bIPty0ABjxoyhT58+VK9enaVLl17Y3qZNG0aMGEH79u0Bp+Ns3bp1lsNAWbFy1yZXVOHXd2DJWKjZDm6bCqUq56qJ1LR0/rN8L+OX7CSkZDE+H9mOaxrlrg2TN1aG2hRKOSl3bb8X+SQtBb57HNZOhmY3w00fQWDuFnyJPHGOx2euZ+2B09zQohqvDWhOiI+uH+wpVobaFClW7roASTgFM4fDvuXQ/Sno8VyuagWpKlNWHeD177YR6C+8c1srBrSq4cGATVYsEZhCx8pdFxAn98HUW50/b/oIWg3L1eFHYxN5evZG/rvzON0aVOStW1pStaw9HewNRSYRqKo9XWguKGxDnoXOgZUwfZizkPzwb6BO11wdvi06lqGfrCQxJY1XBzTjjo617d+vFxWJRBAUFERMTAwVKlSwXyaDqhITE0NQkH279IiNs2DeA1A2FIbNhIr1c3X4ibNJ3DNpDcUD/Jhzf2dbP7gAKBKJoGbNmkRFRXH8+HFvh2IKiKCgIGrWrOntMIoWVfjvm7DsH1C7Cwz5Ckrmblpncmo6938VwYmzScy6r5MlgQKiSCSCwMDAC0+1GmM8IDUJ5j8MG2dAy6HQ7x0IKH754zJQVV74ZhOrI0/x3tDWtoZwAVIkEoExxoPOxcCM2+HA79DzBej2JORhCHbir5HMXBPFwz3r069l3grPGc/w6Nw7EekjIjtEZLeIPJvF57VEZKmIrBORjSLS15PxGGNy6cQu+LQXHFoLt0x0pojmIQn8d+dxXv9uK39pVoXHemf9YKHxHo9dEYiIP/ABcC0QBawWkfmqujXDbi8AM1X1IxFpCiwE6ngqJmNMLuxbDjPuAL9AGPEthLbPUzO7j53loalraVilNONubYWfn03oKGg8eUXQHtitqntVNRmYDgzItI8CZVyvywKHPRiPMSan1n0FX94Mpas5C8nkMQmcjk/mnkmrKebvx6d3hRNc3EajCyJP/l+pARzM8D4K6JBpn5eAH0TkYSAY6J1VQyIyBhgDUKtWLbcHaoxxSU+Hn1+BFf+Gq66BwV9Aibzd1E1NS+ehqes4dDqBaaM7UjPEFpEpqLz9fP5Q4AtVrQn0Bb4UkT/FpKoTVDVcVcMrVaqU70Ea4xOS42HWXU4SaDvSKSGdxyQA8Np321ix+wSv3xxGeB2rHlqQefKK4BAQmuF9Tde2jEYBfQBU9XcRCQIqAsc8GJcxJrO4ozB9qHNT+LrXodODebopfN6UVfv54rdI7ulal1vDQy9/gPEqT14RrAYaiEhdESkG3AbMz7TPAaAXgIg0AYIAeyrMmPx0dIszM+jYNuchsc4PXVES+H1PDGPnbeHqhpX4W1+r/loYeOyKQFVTReQhYDHgD0xU1S0i8gqwRlXnA08An4jIYzg3jkeoFYkxJv/sWgKzRkDxUjDye6je6oqaOxATzwNTIqhdoSTvDWuNv80QKhQ8egtfVRfiTAnNuO3FDK+3Al0yH2eMyQd/fALfPw2Vm8GwGVD2yso/xyWmcM/k1aQrfHpXO8oEBbopUONpNpfLGF+TngaLn4dVH0HDPjDoM+eK4AqkpSuPTl/PnuPnmHx3e+pWDL78QabAsERgjC9JOgtzRsHORdDxAbjuNfDzv+Jm31q8g5+2H+OVAc3oUr+iGwI1+ckSgTG+4swhmDbEuTnc921oP9otzX69NoqP/7uH2zvU4s6Otd3SpslflgiM8QWH18G0oc4VwbBZ0CDLZzdzbe2BUzz79SY6XlWel/o3s/VACilLBMYUddu/gzn3QMkKMGoxVGnmlmYPn05gzOQIqpYJ4qPb2xLo7+3nU01eWSIwpiiLXAHTb4fqrWHodChdxS3NJiSnMebLNSSmpDF1dAdCgou5pV3jHZYIjCmq0tPg+2ehbE2nemgx98zkUVWenLWBLYdj+eyucBpWKe2Wdo33WCIwpqhaPwWObnLWEXBTEgB496fdfLcpmr9d35iejd1zhWG8ywb1jCmKEmPhp1chtAM0G+i2Zr/fFM2/l+xkYJsajOl+ldvaNd5lVwTGFEUrxsG5YzBs+hXVDcpo86EzPD5zA21qleMfN4fZDKEixK4IjClqTkXC7x84i8zXaOuWJo/FJTJm8hrKlQzk4zvbEhR45Q+hmYLDrgiMKWp+fBH8AqDXi5ffNweSUtO478sITsWnMOu+TlQuHeSWdk3BYVcExhQlkb/C1nnQ5VEoU/2Km1NV/vb1JtYeOM2/bm1J8xpl3RCkKWgsERhTVKSnwaJnoUwN6PywW5r85Je9fL32EI/2bkDfsGpuadMUPDY0ZExRsWEaHNkIAz+FYle+PvDP24/yf99v54awajzSs4EbAjQFlV0RGFMUJMXBT69AzXYQdssVN7fraByPTFtPs+pleHtwS/xsgZkiza4IjCkKVoyHs0dhyJQrni566lwyoyatISjQnwl3hlOimM0QKuosERhT2J0+AL+9B2G3Qmi7K2oqJS2d+6dEcCQ2keljOlK9XAk3BWkKMhsaMqaw+3EsiB/0HnvFTb28YAsr957kjYFhtKkV4obgTGFgicCYwuzAStjyNXT5q1Nc7gp8+XskX608wL1XX8XANlfWlilcLBEYU1ilpzvTRUtXhy6PXFFTv+4+wUsLttKrcWWe/ktjNwVoCgu7R2BMYbVxhrPy2M3/uaLqopEnzvHAlLXUqxTM+Nta4W8zhHyOXREYUxglnYWfXobqbZybxHkUm5jCqEmr8RP4dHg7SgcFujFIU1jYFYExhdGv70BcNAyeBH55+z6Xlq48Mm0d+2Pi+XJUB2pVuPKH0EzhZInAmMLm9EH47V1oPghqdchzM298v41lO47zj5vD6FSvghsDNIWNDQ0ZU9j89LLzZ++X8tzErDUH+eSXfdzVqTbDOtRyS1im8LJEYExhcvAP2DTLKSpXLm8d+JrIkzw/dzNd61fk7zc2dXOApjCyRGBMYZGeDov+BqWqOmWm8yDqVDz3fRVBjZASfDCsDQH+1gUYu0dgTOGxeTYcWgM3fQTFS+X68HNJqYyeHEFSajrTh4dTtqTNEDIO+zpgTGGQfM4pJVGtFbS4LdeHp6crT8zcwI4jsbw3tDX1K+c+kZiiy64IjCkMfnsP4g7DLZ/labro+CU7WbTlCH+/sSk9GlX2QICmMLMrAmMKujOHnDLTTW+C2p1zffiCDYd59+fd3Bpek7u71HF/fKbQs0RgTEH308ug6XDty7k+dFPUGZ6ctYF2dUJ49abmyBWuVWCKJksExhRkURFOTaFOD0JInVwdejwuiTFfrqFiqeJ8dEdbigfYAjMmax5NBCLSR0R2iMhuEXk2m31uFZGtIrJFRKZ6Mh5jChVVp7pocGXo9niuDk1KTeP+ryI4FZ/MhOFtqViquIeCNEWBx24Wi4g/8AFwLRAFrBaR+aq6NcM+DYC/AV1U9ZSI2F0sY87bPAei/oD+70Px0jk+TFUZO28La/af4v1hrWlWvawHgzRFgSevCNoDu1V1r6omA9OBAZn2GQ18oKqnAFT1mAfjMabwSElwpotWbQGthuXq0C9X7mf66oM8eE09bmxR3UMBmqLEk4mgBnAww/so17aMGgINReRXEVkpIn2yakhExojIGhFZc/z4cQ+Fa0wB8tv7EBsFff4P/HI+tv/7nhheXrCV3k0q88S1jTwYoClKvH2zOABoAPQAhgKfiEi5zDup6gRVDVfV8EqVKuVziMbks9hoWDEOmvSHOl1zfNjBk/E8MCWCuhWD+feQVvjZAjMmhy6bCESkn4jkJWEcAkIzvK/p2pZRFDBfVVNUdR+wEycxGOO7fnoF0lPh2ldyfIhTPmINaenKJ8PDbYEZkys56eCHALtE5E0Ryc1ipquBBiJSV0SKAbcB8zPt8w3O1QAiUhFnqGhvLs5hTNFyaC1smAod74fydXN0iKry5KwN7Dwax/vD2lC3Yt6XrTS+6bKJQFXvAFoDe4AvROR315j9JacxqGoq8BCwGNgGzFTVLSLyioj0d+22GIgRka3AUuApVY25gp/HmMJL1akuGlwJuj2Z48Pe+3k3328+wnN9m9C9oQ2dmtzL0fRRVY0VkdlACeBR4GbgKRF5V1Xfu8RxC4GFmba9mOG1Ao+7/jPGt22ZCwdXQr93IKhMjg75YcsRxv24k4GtazCqa86uIIzJLCf3CPqLyFxgGRAItFfV64GWwBOeDc8YH5GS6EwXrdIcWt+Zo0N2HInjsRnraVmzLP8YGGblI0ye5eSKYBDwb1VdnnGjqsaLyCjPhGWMj1n5AZw5ADctyNF00dPxyYyevIaSxQP4z53hBAVa+QiTdzlJBC8B0effiEgJoIqqRqrqT54KzBifEXcEfhkHjW+Eut0vu3tqWjoPTV3HkTOJTL+3I1XLBuVDkKYoy8msoVlAeob3aa5txhh3+PlVSE3K8XTRfyzczordJ3jt5ua0qRXi4eCML8hJIghwlYgAwPW6mOdCMsaHHF4P66ZAx/ugQr3L7j5rzUEm/rqPkV3qcGt46GX3NyYncpIIjmeY7omIDABOeC4kY3yEKix+DkpWgO5PXXb3tQdO8fzczXSpX4Hn+8MB2wMAAB5qSURBVDbJhwCNr8jJPYL7gCki8j4gOPWDhns0KmN8wbb5sP9XuGEcBF26QujR2ETu+zKCqmWDeH9oGwL8vV0dxhQll00EqroH6CgipVzvz3o8KmOKupRE+OHvULkptLnrkrsmpqQx5ssIziWl8uWoDoQE28isca8cPVAmIjcAzYCg83OVVTXnhVCMMRdb9RGc3g93zgX/7P8ZqirPfb2JDQdP858729Koas7XJTAmp3LyQNnHOPWGHsYZGhoM1PZwXMYUXWePwfJ/QcProV7PS+762Yp9fL3uEI/1bshfmlXNpwCNr8nJQGNnVR0OnFLVl4FOOMXhjDF58fNrkJoA1712yd2W7zzOPxZu4/rmVXm4Z/18Cs74opwkgkTXn/EiUh1IAap5LiRjirDojbB2MrS/Fypm37nvO3GOh6aupWGV0rw9uKWtLWA8Kif3CBa4Fot5C1gLKPCJR6Mypig6P120RAhcnf100bjEFEZPXoO/n/DJ8HCCi3tsaXFjgMskAteCND+p6mlgjoh8CwSp6pl8ic6YomT7dxD5C/R920kGWUhPVx6bsZ59J87x5aj2hJYvmc9BGl90yaEhVU0HPsjwPsmSgDF5kJoEP7wAlRpD25HZ7jbux50s2XaMF29sSud6FfMxQOPLcnKP4CcRGSRW49aYvFv1Hzi1D/7yerbTRb/deJj3l+7mtnahDO9kE/NM/slJIrgXp8hckojEikiciMR6OC5jio6zx2H5W9DgOqjfO8tdthw+w1OzNtK2dggvD2hmawuYfJWTJ4vtCRZjrsTS1yH5HFz3epYfx5xNYszkCMqVDOTjO9pSPMDWFjD567KJQESyLJCeeaEaY0wWjm6BtZOg/Rio9OfHb5JT07l/ylpOnE1i9n2dqVS6uBeCNL4uJ/PSMs5zCwLaAxHApR+JNMbXnV+MvngZuPqZLHd55dst/LHvJO/c1oqwmpcuPGeMp+RkaKhfxvciEgqM91hExhQVOxfBvv/C9W9CyfJ/+njKqv18tfIA9159FQNa1fBCgMY48lLLNgqwYujGXEpqMix+Hio2hPC7//TxH/tOMnbeFno0qsTTf2nshQCN+Z+c3CN4D+dpYnASRyucJ4yNMdlZ/Qmc3AO3zwb/wIs+OnQ6gfu/iqBW+ZK8c1tr/K18hPGynNwjWJPhdSowTVV/9VA8xhR+52Jg2T+dqaINrr3oo4TkNMZMXkNyajqf3BVO2RKB2TRiTP7JSSKYDSSqahqAiPiLSElVjfdsaMYUUsv+Acln/zRdVFV5avYGtkbHMvGudtSrVMpLARpzsRw9WQyUyPC+BLDEM+EYU8gd3QprJjr3BSpfPPb/4bI9fLsxmqf/0phrGlf2UoDG/FlOEkFQxuUpXa+tEpYxmZ2vLlq8NPT420Uf/bTtKG//sIP+Latz39VXeSlAY7KWk0RwTkTanH8jIm2BBM+FZEwhtesH2LsUrn4Wgitc2Lz7WBx/nb6eZtXL8M9BLax8hClwcnKP4FFglogcxlmqsirO0pXGmPMSzzjTRSvUh3b3XNh8Jj6F0ZMjCAr0Y8Kd4ZQoZuUjTMGTkwfKVotIY6CRa9MOVU3xbFjGFAIpibBrMWyaDTsXQ1oSDJsJAcUASEtXHp6+jqhT8Uwd3ZHq5UpcpkFjvCMnzxE8CExR1c2u9yEiMlRVP/R4dMYUNGmpELnc6fy3LYCkWAiuBOEjocWtUKPthV3/uWg7y3ce5/8GhtGuzp+fLDamoMjJ0NBoVc24OM0pERkNWCIwvkEVotbAplmwZS6cO+bUD2rSD8JugTrd/7TGwNx1UUxYvpfhnWoztH0tLwVuTM7kJBH4i4ioqoLzHAFQzLNhGVMAHNvudP6bZ8OpSPAvDg3/AmGDnbUFAoOyPGzDwdM8M2cTHa8qz99vbJq/MRuTBzlJBIuAGSLyH9f7e4HvPReSMV50+gBsngOb5sDRTSB+UPdq6P40NLkRgi5dIfRYbCJjvlxDpVLF+fD2tgT656WclzH5KyeJ4BlgDHCf6/1GnJlDxhQN52Jg61xn3P/A7862mu2cqqFNb4LSVXLUTFJqGvd9FUFsQipz7u9M+WC7cDaFQ05mDaWLyCqgHnArUBGYk5PGRaQP8A7gD3yqqm9ks98gnFIW7VR1TVb7GONWSXGwfaEz7LPnZ0hPhYqNoOcL0HwQlM/dQ1+qygtzN7P2wGk+vL0NTauX8VDgxrhftolARBoCQ13/nQBmAKjqNTlp2HUv4QPgWpzS1atFZL6qbs20X2ngr8CqvPwAxuRYajLsXuKM++/4HlIToExN6PSgM+5fpTnk4WEvVeWtxTuYFRHFIz3r0zesmgeCN8ZzLnVFsB34BbhRVXcDiMhjuWi7PbBbVfe6jp0ODAC2ZtrvVeCfXLwSmjHukZ4G+39zOv+t8yDxNJQoD62GOZ1/aAfwy/s4vqry6rfbmPjrPoa2D+XR3n9ejtKYgu5SiWAgcBuwVEQWAdNxnizOqRrAwQzvo4AOGXdwla4IVdXvRCTbRCAiY3DuU1Crlk3FM5ehCtHrnTH/zXMgLhoCg6HxDU7nX++aP60RkBfp6crf521myqoDjOhch7H9mlr5CFMoZZsIVPUb4BsRCcb5Jv8oUFlEPgLmquoPV3JiEfEDxgEjLrevqk4AJgCEh4frZXY3vurEbmfMf9MsiNkNfoHOegBhr0PDPlAs2G2nSktXnpmzkdkRUdx3dT2e6dPIkoAptHJys/gcMBWYKiIhwGCcmUSXSwSHgNAM72u6tp1XGmgOLHP9A6oKzBeR/nbD2ORY7GHY/LXT+UevBwTqdIXOD0OT/lmuFXylUtLSeXzmBhZsOMyjvRvw114NLAmYQi0n00cvUNVTON/MJ+Rg99VAAxGpi5MAbgOGZWjrDM4MJABEZBnwpCUBc1kJp2DrfKfzj1wBKFRr5SwE03wglKnusVMnpabxyLR1LN5ylGf6NOb+HvU8di5j8kuuEkFuqGqqiDwELMaZPjpRVbeIyCvAGlWd76lzmyJs63yYc49T4K18Pbj6GafMQ8UGHj91Ykoa938VwdIdxxnbrykju9T1+DmNyQ8eSwQAqroQWJhp24vZ7NvDk7GYIuDgH/D1aKjWwnnYq3rrPE33zIv45FTumbSG3/fG8I+bwxjWwSYtmKLDo4nAGLeJ2QPTbnOGfYbOuGjhF0+LS0zh7i9WE7H/FG/f0pJBbWvm27mNyQ+WCEzBF38Spgx2poXePjtfk8CZ+BSGf/4HWw6d4d2hrbmxhefuPxjjLZYITMGWkgjTh8GZKLhrPlTIv5uzMWeTuPOzP9h97Cwf3t6G65pZiS1TNFkiMAVXejrMe8ApBHfLRKjVMd9OfSwukds/WcWBk/FMGN6WHo0q59u5jclvlghMwfXzq86Twb1fcgrB5ZPoMwnc/skqjsQm8vnIdnSuV/HyBxlTiFkiMAVTxBewYhy0HQFdHs230x48Gc+wT1dy6lwKk+9uT7gtMWl8gCUCU/DsXgLfPg71e0Pff+XbFNF9J84x7JOVxCenMeWeDrQMLZcv5zXG2ywRmILlyCaYOQIqN4XBX/xpLWBP2XU0jmGfriItXZk2uqOtJ2B8iiUCU3DEHoYpt0Lx0jBshvNnPthy+Ax3fvYHAX7CjDEdaVAlf85rTEFhicAUDElxThJIioW7F0HZGvly2g0HT3PnZ6soVTyAKaM7Urei+yqUGlNYWCIw3peWCrNGwLGtMGwmVA3Ll9OujjzJyM9XExIcyNR7OhJavmS+nNeYgsYSgfEuVVj4hHOD+Mbx0KB3vpz2t90nGDVpDdXKBjF1dEeqlg3Kl/MaUxDlfY0+Y9zh13ecqaJdH4PwkflyymU7jjHyi9WEli/B9HstCRhjVwTGezZ/DUvGQrOB0DPLorRu98OWIzw4dS0Nq5Tmy1EdKB9cLF/Oa0xBZonAeMeBlTD3PgjtCDd9dEULyOfUgg2HeWzGeprXKMukke0pW/LK1y02piiwRGDyX8wemDYUytaEodMg0PNDM7Mjonh69gbCa5dn4sh2lCpuv/rGnGf3CEz+OhcDU25xnha+fZZH1hTObOqqAzw5awOd61Xki7stCRiTmf2LMPknJRGmD4Uzh2DEt/lSUvrzX/fx8oKt9GxcmQ9vb0NQoL/Hz2lMYWOJwOSP9HT45j44uAoGT4LQ9h4/5UfL9vDPRdvp06wq7w5tTbEAuwA2JiuWCEz++Oll2DIXrn0Vmt3k0VOpKuOX7OKdn3bRv2V1xt3akgB/SwLGZMcSgfG8NRPh1/EQPgo6P+zRU6kqbyzazn/+u5fBbWvyxqAW+PvlT/VSYworSwTGs3b9CN89CQ2ug+vf9GhJaVXl5QVb+eK3SO7oWItX+jfHz5KAMZdlicB4TvRGp4ZQlWZwy+ceLSmdnq48/80mpv1xkHu61uX5G5og+bSOgTGFnSUC4xlnDsHUWyGorFNIrngpj50qNS2dp2dv5Ot1h3jomvo8cV1DSwLG5IIlAuN+ibFOEkg6C6MWQ5lqHjtVSlo6j05fz3ebonnyuoY81LOBx85lTFFlicC4V1qKMxx0fLvzwFiVZh47VVJqGg9NXcePW4/ywg1NuKfbVR47lzFFmSUC4z6q8N3jsOcn6P8e1OvpsVMlJKdx71cRLN95nFcHNOPOTnU8di5jijpLBMZ9Vvwb1k6Gbk9Cm+EeO825pFRGTVrNqn0neXNQC25tF+qxcxnjCywRGPfYNNt5aCxsMPR8wWOnOXkumdGT17D+4GnGD2nFgFb5s6SlMUWZJQJz5fb/Dt/cD7U6w4APPPaswKLN0bzwzWbOJKTw/tDWXB/muZvQxvgSSwTmypzY7RSSK1cbbpsCAcXdfoqYs0mMnb+FbzdG07xGGb4c1YEm1cq4/TzG+CpLBCbvzp1wlZT291hJ6YWbovn7N5uJTUzhyesacu/V9Qi0ukHGuJUlApM3KQnO4jJx0XDXt1C+rlubP3E2ibHztvDdpmjCapRl6uCONKpa2q3nMMY4LBGY3EtPh7n3QtRquHUShLZza/PfbjzMi/O2cDYxlaf+0oh7u19l1UON8SCPJgIR6QO8A/gDn6rqG5k+fxy4B0gFjgN3q+p+T8Zk3GDJWNg6D657HZoOcFuzx+OSeHHeZr7ffISWNcvy1uCWNKxiVwHGeJrHEoGI+AMfANcCUcBqEZmvqlsz7LYOCFfVeBG5H3gTGOKpmIwbrP4UfnsX2o2GTg+6pUlVZcHGaMbO28y5pDSe6dOY0d3q2lWAMfnEk1cE7YHdqroXQESmAwOAC4lAVZdm2H8lcIcH4zFXaucPsPApaNgH+rzhlmmix+IS+fs3m1m85SgtQ8vx9i0taGBXAcbkK08mghrAwQzvo4AOl9h/FPB9Vh+IyBhgDECtWrXcFZ/JjegNTg2hqmEw6LMrLimtqszfcJix87cQn5zG365vzKiudhVgjDcUiJvFInIHEA5cndXnqjoBmAAQHh6u+RiaATgTBVNuhRIhbikpfSwukRfmbuaHrUdpFVqOtwe3oH5luwowxls8mQgOARmLwNR0bbuIiPQGngeuVtUkD8Zj8iLxDEwZDCnxcPdiKF01z02pKvPWO1cBiSlpPNe3MaO6XmVLSRrjZZ5MBKuBBiJSFycB3AYMy7iDiLQG/gP0UdVjHozF5EVaCsy8C07shNtnQ5WmeW7qWGwiz83dzJJtR2lTqxxvDW5JvUqeW6zGGJNzHksEqpoqIg8Bi3Gmj05U1S0i8gqwRlXnA28BpYBZrhWlDqhqf0/FZHJBFb59DPYudeoH1bsmj80oc9cd4qX5W0hKTeeFG5owsktduwowpgDx6D0CVV0ILMy07cUMr3t78vwmj+KOwsInYdt86P40tM7bZK6jsYk89/Umftp+jPDaIbx5SwuusqsAYwqcAnGz2BQQqrBhOix61ikh0WssdH0sD80oc9Ye4pUFW0hOS+fvNzZlROc6dhVgTAFlicA4Th+Ebx+F3UsgtKOzwlilhrlu5siZRP729UaW7jhOuzohvHlLS+pWDPZAwMYYd7FE4OvS02HNZ7DkJeeK4Po3naeG/XI3n19VmRURxavfbiUlLZ2x/ZpyV6c6+NlVgDEFniUCXxazB+Y/DPt/hat6QL93IaR2rpuJPpPAs3M28d+dx2lftzxvDmpBHbsKMKbQsETgi9JSYeUHsPQf4F8c+r/v3BDOZckIVWXmmoO89u02UtOVl/o1ZbhdBRhT6Fgi8DVHt8C8B+HwOmh8I/R9G8rkfsnHQ6cTeHbORn7ZdYIOdcvz5i0tqF3BrgKMKYwsEfiK1GT45W345V8QVA5u+Rya3Zynq4Dpqw/y+nfbSFfllQHNuKNDbbsKMKYQs0TgC6IinKuA49sg7Fancmhwhdw3cyqev329iV92naDjVeV5c1BLalUo6YGAjTH5yRJBUZYcD0tfh5UfQqmqTsG4hn/JdTOqyrQ/DvKPhc5VwKs3Nef29rXsKsCYIsISQVEVucKZEXRyL7QdCde+DEFlc93MwZPOVcCK3SfoXK8C/xzUgtDydhVgTFFiiaCoSYx1lpJcMxFC6sBdC6Bu91w3k5qWzvTVB/m/hdsAeP3m5gxrXwtxw2I0xpiCxRJBUbLzB+fp4Lho6PQQXPM8FMv5t/fUtHRW7TvJd5uiWbz5CDHnkulavyJvDAqjZohdBRhTVFkiKAriTzr1gTbOgEqN4dbJUDM8R4empqWzcq/T+f+wxen8Sxbzp2fjyvRrWZ3rmlaxqwBjijhLBIWZKmyZ66wjnHjaqRTa/UkIKH7Jw1LT0vl9bwwLN0WzeMtRTro6/15NqnBDWFWubliZEsX88+mHMMZ4myWCwiruCHz3BGz/Fqq1guHfOOsJZyMlLZ3f95zv/I9wKj6FYFfn3zesGj0aVSIo0Dp/Y3yRJYLCRhXWT4HFz0FKIvR+2bkfkMVi8ilp6fy2J4aFG6NZvPUIp12df++mTud/dUPr/I0xlggKl1P7nZvBe36GWp2cGkEV61+0S0paOr/uPsHCTdH8sPUop+NTKFU8gN5NKtM3rBrdrfM3xmRiiaAwSE+H1Z86paJFnPpA4aMulIpOTk3n1z0nWLjR6fzPJPyv87+hRXW6Nahonb8xJluWCAq6E7tg3kNwcCXU6wn93oFytZzOf9cxvtsYzY+uzr908YALwz7W+RtjcsoSQUGVlgq/vQvL3oDAIBjwIcnNb+PXPTF89+MGfthyhNjEVEoXD+Da851/w4oUD7DO3xiTO5YICqIjm5wicdEbSG/cj98aPcvcXWn8OG+J0/kHOZ3/DWHV6NrAOn9jzJWxRFCQpCbB8rfQFf8mJbAsU2u+zL+2NyFu/X5KBwVwXdOq3NCiKl3qW+dvjHEfSwQFRHLkSpK/foBSsXtYoN15Mf520pNC+EuzqtwQVo0u9StSLCB36wgbY0xOWCLIZ6pKbEIqB0/FE3UqgVPRe6i+7XO6xczmBCE8I3+jZPPr+XeLanSpZ52/McbzLBF4wJmEFKJcHX3UqQQOnjz/Op60UwcJS9lIR79tdPTbSi2/4wD8VuEm0nq+yL8b17XO3xiTrywR5EFsYkqGzj3hok4/6lQ8cYmpF/atRgxXF9vO4KCdtE7fTGWJhmKQXKwsCdU7kVCvO0GNetG5cmMv/kTGGF9miSALsYkpRJ38cwd//s/YDB09QMli/oSGlKRmSAl6V0+hje6kQfw6KsespljcAWcnv3JwVVeo0xXqdKNY5aYU87Nv/sYY7/PJRBCXmHJRB3/w5OU7+pohJQgNKUm7OiHUdHX6NUNKEhpwirJHVyKR3zqrgkXucw4KKufq9B9w/qzc7MKTwMYYU5D4TCKYuy6KT3/ZR9SpBM4kpFz0WYlAf0LLOx17eJ2QC538+T9DSgb+ryZ/7GGnw9/3Cyxd4SwFCc4ykLW7QvsxTsdfpbl1/MaYQsFnEkHxAH8qly5Om1oXd/Sh5TN19JnFRsOmFRD5i5MATu5xtgeVhdpdoN09UKcbVGkGfja33xhT+PhMIugbVo2+YdUuv2NsNOz/FfYtv7jjL14WaneGdqMyfOO3jt8YU/j5TCLIVtwR19i+6xt/zG5n+/mOP/xup+OvGmYdvzGmSPK9RHCh43f9F7PL2V68jNPxtx3hDPVYx2+M8RG+kwg2zoTlb8GJnc774mWcxV3a3uX6xt/COn5jjE/ynUQQEAQhdaH1nf/r+LNY3tEYY3yNR3tCEekDvAP4A5+q6huZPi8OTAbaAjHAEFWN9EgwTfs7/xljjLmIxya6i4g/8AFwPdAUGCoiTTPtNgo4par1gX8D//RUPMYYY7LmySee2gO7VXWvqiYD04EBmfYZAExyvZ4N9JJsJ/QbY4zxBE8ODdUADmZ4HwV0yG4fVU0VkTNABeBExp1EZAwwxvX2rIjsyGNMFTO3XUBYXLljceVeQY3N4sqdK4mrdnYfFIq7pao6AZhwpe2IyBpVDXdDSG5lceWOxZV7BTU2iyt3PBWXJ4eGDgGhGd7XdG3Lch8RCQDK4tw0NsYYk088mQhWAw1EpK6IFANuA+Zn2mc+cJfr9S3Az6qqHozJGGNMJh4bGnKN+T8ELMaZPjpRVbeIyCvAGlWdD3wGfCkiu4GTOMnCk654eMlDLK7csbhyr6DGZnHljkfiEvsCbowxvs0K5htjjI+zRGCMMT7OZxKBiPQRkR0isltEnvV2PAAiMlFEjonIZm/HkpGIhIrIUhHZKiJbROSv3o4JQESCROQPEdngiutlb8eUkYj4i8g6EfnW27GcJyKRIrJJRNaLyBpvx3OeiJQTkdkisl1EtolIpwIQUyPX39P5/2JF5FFvxwUgIo+5fuc3i8g0EQlya/u+cI/AVe5iJ3AtzoNtq4GhqrrVy3F1B84Ck1W1uTdjyUhEqgHVVHWtiJQGIoCbCsDflwDBqnpWRAKBFcBfVXWlN+M6T0QeB8KBMqp6o7fjAScRAOGqWqAejhKRScAvqvqpa1ZhSVU97e24znP1GYeADqq638ux1MD5XW+qqgkiMhNYqKpfuOscvnJFkJNyF/lOVZfjzJYqUFQ1WlXXul7HAdtwngL3KnWcdb0NdP1XIL7JiEhN4AbgU2/HUtCJSFmgO86sQVQ1uSAlAZdewB5vJ4EMAoASruetSgKH3dm4rySCrMpdeL1jKwxEpA7QGljl3UgcruGX9cAx4EdVLRBxAeOBp4F0bweSiQI/iEiEq1RLQVAXOA587hpK+1REgr0dVCa3AdO8HQSAqh4C3gYOANHAGVX9wZ3n8JVEYPJAREoBc4BHVTXW2/EAqGqaqrbCeVK9vYh4fUhNRG4EjqlqhLdjyUJXVW2DUwX4QddwpLcFAG2Aj1S1NXAOKBD37QBcQ1X9gVnejgVAREJwRjDqAtWBYBG5w53n8JVEkJNyFyYD1xj8HGCKqn7t7Xgycw0lLAX6eDsWoAvQ3zUePx3oKSJfeTckh+vbJKp6DJiLM0zqbVFAVIarudk4iaGguB5Yq6pHvR2IS29gn6oeV9UU4GugsztP4CuJICflLoyL66bsZ8A2VR3n7XjOE5FKIlLO9boEzs3/7d6NClT1b6paU1Xr4Pxu/ayqbv3GlhciEuy62Y9r6OU6wOsz1FT1CHBQRBq5NvUCvDoRIZOhFJBhIZcDQEcRKen6t9kL576d2xSK6qNXKrtyF14OCxGZBvQAKopIFDBWVT/zblSA8w33TmCTazwe4DlVXejFmACqAZNcMzr8gJmqWmCmahZAVYC5riU+AoCpqrrIuyFd8DAwxfXFbC8w0svxABcS5rXAvd6O5TxVXSUis4G1QCqwDjeXmvCJ6aPGGGOy5ytDQ8YYY7JhicAYY3ycJQJjjPFxlgiMMcbHWSIwxhgfZ4nAmExEJC1TFUq3PfUqInUKWrVZY3ziOQJjcinBVcbCGJ9gVwTG5JCrtv+brvr+f4hIfdf2OiLys4hsFJGfRKSWa3sVEZnrWj9hg4icLwvgLyKfuOrL/+B6StoYr7FEYMyflcg0NDQkw2dnVDUMeB+n4ijAe8AkVW0BTAHedW1/F/ivqrbEqaVz/mn2BsAHqtoMOA0M8vDPY8wl2ZPFxmQiImdVtVQW2yOBnqq611WU74iqVhCREzgL+aS4tkerakUROQ7UVNWkDG3UwSmf3cD1/hkgUFVf8/xPZkzW7IrAmNzRbF7nRlKG12nYvTrjZZYIjMmdIRn+/N31+jecqqMAtwO/uF7/BNwPFxbUKZtfQRqTG/ZNxJg/K5Gh6irAIlU9P4U0REQ24nyrH+ra9jDOaltP4ay8db6S5l+BCSIyCueb//04K0wZU6DYPQJjcqigLgRvzJWyoSFjjPFxdkVgjDE+zq4IjDHGx1kiMMYYH2eJwBhjfJwlAmOM8XGWCIwxxsf9PwT2fSni82NNAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiFZ0cSb7NRm","executionInfo":{"status":"ok","timestamp":1633460828551,"user_tz":-330,"elapsed":14666,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"76c97af4-3067-4d69-8e5c-65dd0955d2a1"},"source":["test_acc, _ = eval_model(\n","  trainedmodel,\n","  test_data_loader,\n","  loss_fn,\n","  device,\n","  len(df_test)\n",")\n","test_acc"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(0.0088, device='cuda:0', dtype=torch.float64)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"YaOzcKOQ8JTF"},"source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  review_texts = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","  with torch.no_grad():\n","    for d in data_loader:\n","      texts = d[\"review_text\"]\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","      review_texts.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(outputs)\n","      real_values.extend(targets)\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return review_texts, predictions, prediction_probs, real_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVb0VWqZ9KvT","executionInfo":{"status":"ok","timestamp":1633460761454,"user_tz":-330,"elapsed":9430,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"b51afb9a-1630-4eb8-d64d-93d2f6c5e171"},"source":["y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","  trainedmodel,\n","  test_data_loader\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmrScHyh9QP9","executionInfo":{"status":"ok","timestamp":1633460761456,"user_tz":-330,"elapsed":60,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"6107aee0-5879-4484-e6a9-a9a0965d0e47"},"source":["print(classification_report(y_test, y_pred, target_names=class_names))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                               precision    recall  f1-score   support\n","\n","            Andriod_Developer       0.08      0.70      0.15        10\n","             Python_Developer       0.00      0.00      0.00         4\n","           Front_Enddeveloper       0.00      0.00      0.00         7\n","Oracle_Database_Administrator       0.00      0.00      0.00         5\n","                 UI_Developer       0.00      0.00      0.00         1\n","          Full_Stackdeveloper       0.00      0.00      0.00         3\n","                Web_Developer       0.00      0.00      0.00         3\n","           Software_Developer       0.00      0.00      0.00         4\n","       Database_Administrator       0.00      0.00      0.00         3\n","                      Dot_Net       0.00      0.00      0.00         6\n","        Network_Administrator       0.00      0.00      0.00         3\n","             Security_Analyst       0.00      0.00      0.00         2\n","        Application_Developer       0.00      0.00      0.00         8\n","            Software_Engineer       0.00      0.00      0.00         3\n","       Cyber_Security_Analyst       0.00      0.00      0.00         1\n","         System_Administrator       0.00      0.00      0.00         3\n","  SQL_Database_Administration       0.00      0.00      0.00         6\n"," Infromation_Security_Analyst       0.00      0.00      0.00         5\n","              Project_Manager       0.00      0.00      0.00         4\n","                   IT_Analyst       0.00      0.00      0.00         7\n","       PythonDjango_Developer       0.00      0.00      0.00         3\n","         Salesforce_Developer       0.00      0.00      0.00         5\n","               Java_Developer       0.00      0.00      0.00         2\n","    Data_System_Administrator       0.00      0.00      0.00         2\n","            Linux_SystemAdmin       0.00      0.00      0.00         4\n","             Hadoop_Developer       0.00      0.00      0.00         5\n","           IT_Program_Manager       0.00      0.00      0.00         3\n","                IOS_developer       0.00      0.00      0.00         2\n","\n","                     accuracy                           0.06       114\n","                    macro avg       0.00      0.02      0.01       114\n","                 weighted avg       0.01      0.06      0.01       114\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"id":"WEfn2Ti7CsWf"},"source":["import codecs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urhRbehgAC7Z","executionInfo":{"status":"ok","timestamp":1633460761457,"user_tz":-330,"elapsed":51,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"5dca87f2-4c27-4c1e-ae05-bcfff685e010"},"source":["trial = codecs.open(\"/content/drive/MyDrive/jalop/bigdata.txt\", \"rU\", encoding='utf-8', errors='ignore')\n","trial_file = trial.read()\n","\n","encoding_test12 = tokenizer.encode_plus(\n","  trial_file,\n","  add_special_tokens=True,\n","  max_length=512,\n","  return_token_type_ids=False,\n","  pad_to_max_length=True,\n","  return_attention_mask=True,\n","  return_tensors='pt',\n",")\n","print(encoding_test12[\"input_ids\"].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 512])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iR2F2jrtC-so","executionInfo":{"status":"ok","timestamp":1633460761457,"user_tz":-330,"elapsed":43,"user":{"displayName":"Anirudh Shivshetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0stRBVSjyvTuYQSm06I1Pb8vSSH6vN1mosnY_cg=s64","userId":"12956648355569486471"}},"outputId":"5a491190-e562-429c-a125-83749063c607"},"source":["input_ids1 = encoding_test12['input_ids'].to(device)\n","attention_mask1 = encoding_test12['attention_mask'].to(device)\n","output12 = trainedmodel(input_ids1, attention_mask1)\n","_, prediction = torch.max(output12, dim=1)\n","\n","print(f'Class  : {class_names[prediction]}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class  : Andriod_Developer\n"]}]}]}